{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import d4rl\n",
    "import gym\n",
    "import numpy as np\n",
    "import wandb\n",
    "import collections\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import discount_cumsum, D4RLTrajectoryDataset, evaluate_on_env, get_d4rl_normalized_score\n",
    "from model import MaskedCausalAttention, Block, DecisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment\n",
    "# sys.path.append(r'C:\\Develop\\offlineRL-with-diffusion') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mujoco-py check passed\n",
      "d4rl check passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Apr 30 2024 12:01:25\n",
      "c:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\gym\\spaces\\box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# test mujoco, d4rl\n",
    "\n",
    "!python ./test/mujoco_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data download\n",
    "# if you downloaded, don't re-start.\n",
    "\n",
    "# !python ./data/download_d4rl_datasets.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "\n",
    "env_name = 'halfcheetah'\n",
    "dataset = 'medium'\n",
    "\n",
    "if env_name == 'hopper':\n",
    "    env = gym.make('Hopper-v3')\n",
    "    max_ep_len = 1000\n",
    "    # env_targets = [3600, 1800]  # evaluation conditioning targets\n",
    "    scale = 1000.  # normalization for rewards/returns\n",
    "elif env_name == 'halfcheetah':\n",
    "    env = gym.make('HalfCheetah-v3')\n",
    "    max_ep_len = 1000\n",
    "    # env_targets = [12000, 6000]\n",
    "    scale = 1000.\n",
    "elif env_name == 'walker2d':\n",
    "    env = gym.make('Walker2d-v3')\n",
    "    max_ep_len = 1000\n",
    "    # env_targets = [5000, 2500]\n",
    "    scale = 1000.\n",
    "    \n",
    "DATA_PATH = f'data/{env_name}-{dataset}-v2.pkl'\n",
    "LOG_PATH = \"./log/\"\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  [[ 1.9831914e-02 -8.9501314e-02 -3.1969063e-03 ...  1.1365079e-01\n",
      "   6.8424918e-02 -1.3811582e-01]\n",
      " [-3.8486063e-03 -5.2394319e-02  8.3050327e-03 ...  4.5068407e+00\n",
      "  -9.2885571e+00  4.7328596e+00]\n",
      " [-5.5298433e-02 -7.7850236e-05 -2.3952831e-01 ... -7.0811687e+00\n",
      "  -1.4037068e+00  7.5524049e+00]\n",
      " ...\n",
      " [-3.1975684e-01  5.3305399e-01 -4.8704177e-01 ...  1.5455554e+00\n",
      "   2.6812897e+00  8.7905388e+00]\n",
      " [-3.2200974e-01  3.5745117e-01  1.0463273e-02 ... -6.3428599e-01\n",
      "   1.6292539e+00  9.7356015e-01]\n",
      " [-3.0673215e-01  1.9843711e-01  6.9996923e-01 ...  5.0098950e-01\n",
      "   1.5680059e+00  9.4733723e-02]] \n",
      "\n",
      "next_state:  [[-3.8486063e-03 -5.2394319e-02  8.3050327e-03 ...  4.5068407e+00\n",
      "  -9.2885571e+00  4.7328596e+00]\n",
      " [-5.5298433e-02 -7.7850236e-05 -2.3952831e-01 ... -7.0811687e+00\n",
      "  -1.4037068e+00  7.5524049e+00]\n",
      " [-1.2996776e-01  2.2959358e-03 -2.2985412e-01 ... -7.0144100e+00\n",
      "   2.6917322e+00 -1.6729002e+00]\n",
      " ...\n",
      " [-3.2200974e-01  3.5745117e-01  1.0463273e-02 ... -6.3428599e-01\n",
      "   1.6292539e+00  9.7356015e-01]\n",
      " [-3.0673215e-01  1.9843711e-01  6.9996923e-01 ...  5.0098950e-01\n",
      "   1.5680059e+00  9.4733723e-02]\n",
      " [-2.4109586e-01  1.8103543e-01  6.5954477e-01 ... -6.4027554e-01\n",
      "  -4.5139942e+00 -5.8575149e+00]] \n",
      "\n",
      "state:  [[ 4.7026437e-02 -2.1588113e-02  4.9151547e-02 ...  5.5219561e-02\n",
      "  -1.5351681e-01 -4.6239123e-02]\n",
      " [ 4.1392505e-02  5.3802542e-02 -1.5022255e-01 ...  6.1133021e-01\n",
      "  -7.4645710e+00  7.9509692e+00]\n",
      " [ 9.8547200e-04  8.8533267e-02 -4.3876743e-01 ...  8.5824745e-04\n",
      "   5.9796906e+00  4.9521341e+00]\n",
      " ...\n",
      " [-1.4081973e-01 -7.7957302e-02 -2.6429656e-01 ...  1.0316861e+00\n",
      "  -7.5645506e-01 -6.3285580e+00]\n",
      " [-6.0340445e-02 -4.2597357e-02  1.1770165e-01 ... -1.8022682e+01\n",
      "  -2.8895503e-01 -8.5740490e+00]\n",
      " [-3.9826483e-02  8.9701705e-02  4.8491257e-01 ... -1.5949978e+01\n",
      "  -4.2975497e+00  3.4759271e-01]] \n",
      "\n",
      "next_state:  [[ 4.1392505e-02  5.3802542e-02 -1.5022255e-01 ...  6.1133021e-01\n",
      "  -7.4645710e+00  7.9509692e+00]\n",
      " [ 9.8547200e-04  8.8533267e-02 -4.3876743e-01 ...  8.5824745e-04\n",
      "   5.9796906e+00  4.9521341e+00]\n",
      " [-6.7215808e-02  9.6443370e-02 -5.4310286e-01 ...  3.3308443e-01\n",
      "  -9.9926096e-01 -1.0109675e+00]\n",
      " ...\n",
      " [-6.0340445e-02 -4.2597357e-02  1.1770165e-01 ... -1.8022682e+01\n",
      "  -2.8895503e-01 -8.5740490e+00]\n",
      " [-3.9826483e-02  8.9701705e-02  4.8491257e-01 ... -1.5949978e+01\n",
      "  -4.2975497e+00  3.4759271e-01]\n",
      " [ 3.2575438e-03  1.4370975e-01  2.0129907e-01 ...  1.7884182e+01\n",
      "   3.8814716e+00  1.0334390e+01]] \n",
      "\n",
      "state:  [[ 5.0221119e-02 -2.4338657e-02 -5.4945849e-02 ... -1.4283662e-01\n",
      "  -1.2819463e-01 -1.4791407e-01]\n",
      " [ 3.4841169e-02  2.6000816e-02 -2.3993976e-01 ... -2.5026150e+00\n",
      "  -9.1679888e+00  6.2272105e+00]\n",
      " [-2.2413606e-02  5.1811527e-02 -5.4271245e-01 ... -5.9999785e+00\n",
      "  -6.6550332e-01 -2.4636595e-01]\n",
      " ...\n",
      " [-4.4023961e-02 -2.5939721e-01 -7.0705950e-02 ... -1.8107307e+01\n",
      "  -4.7313199e+00 -4.8486176e+00]\n",
      " [-1.7510138e-02 -2.1372417e-01  2.7898479e-02 ...  2.9009397e+00\n",
      "  -2.2113817e+00 -5.9028540e+00]\n",
      " [ 4.7847845e-02 -1.5627538e-01 -1.8005599e-01 ...  9.9821644e+00\n",
      "  -5.1808944e+00  1.1870988e+01]] \n",
      "\n",
      "next_state:  [[ 0.03484117  0.02600082 -0.23993976 ... -2.502615   -9.167989\n",
      "   6.2272105 ]\n",
      " [-0.02241361  0.05181153 -0.54271245 ... -5.9999785  -0.6655033\n",
      "  -0.24636595]\n",
      " [-0.07972456  0.05383781 -0.20385133 ... -3.1521757   0.49475375\n",
      "  -1.1187115 ]\n",
      " ...\n",
      " [-0.01751014 -0.21372417  0.02789848 ...  2.9009397  -2.2113817\n",
      "  -5.902854  ]\n",
      " [ 0.04784784 -0.15627538 -0.18005599 ...  9.982164   -5.1808944\n",
      "  11.870988  ]\n",
      " [ 0.05458786 -0.11922822 -0.45288673 ...  3.2934191   2.8909256\n",
      "  12.051488  ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data check\n",
    "# check three trajectories\n",
    "\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "    trajectories = pickle.load(f)\n",
    "n=0\n",
    "max_rewards_list = []\n",
    "for traj in trajectories:\n",
    "    # print(f\"{n+1}번째 trajectory\")\n",
    "    print(\"state: \", traj['observations'], \"\\n\")\n",
    "    # print(\"action: \", traj['actions'], \"\\n\")\n",
    "    print(\"next_state: \", traj['next_observations'], \"\\n\")\n",
    "    # print(\"reward: \", traj['rewards'], \"\\n\")\n",
    "    # print(\"max_rewards: \", max(traj['rewards']))\n",
    "    # max_rewards_list.append(max(traj['rewards']))\n",
    "    # print(\"\")\n",
    "    n+=1\n",
    "    \n",
    "# print(max(max_rewards_list))\n",
    "\n",
    "    if n==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train parameter\n",
    "batch_size = 64\n",
    "embed_dim = 128\n",
    "activation = 'relu'\n",
    "drop_out = 0.1\n",
    "k = 20\n",
    "n_blocks = 3    \n",
    "n_heads = 1 # transformer head\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = 200\n",
    "num_updates_per_iter = 100\n",
    "total_updates = 0\n",
    "max_d4rl_score = -1.0\n",
    "\n",
    "wt_decay = 1e-4             # weight decay\n",
    "lr = 1e-4                   # learning rate\n",
    "warmup_steps = 10000        # warmup steps for lr scheduler\n",
    "\n",
    "# weight of mse loss\n",
    "state_weight = 1\n",
    "reward_weight = 1\n",
    "\n",
    "# evaluation parameter\n",
    "max_eval_ep_len = 1000      # max len of one evaluation episode\n",
    "num_eval_ep = 10            # num of evaluation episodes per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessing(normalization, fit padding) data\n",
    "\n",
    "traj_dataset = D4RLTrajectoryDataset(DATA_PATH, k)\n",
    "traj_data_loader = DataLoader(traj_dataset,\n",
    "\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "                        \n",
    "data_iter = iter(traj_data_loader)\n",
    "\n",
    "## get state stats from dataset\n",
    "state_mean, state_std = traj_dataset.get_state_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim:  17\n",
      "action dim:  6\n"
     ]
    }
   ],
   "source": [
    "# make environment\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"state dim: \", state_dim)\n",
    "print(\"action dim: \", act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps shape:  torch.Size([4, 2])\n",
      "rewards shape:  torch.Size([4, 2, 1])\n",
      "states shape:  torch.Size([4, 2, 17])\n",
      "actions shape:  torch.Size([4, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "temp_dataset = D4RLTrajectoryDataset(DATA_PATH, 2)\n",
    "temp_data_loader = DataLoader(temp_dataset,\n",
    "\t\t\t\t\t\tbatch_size=4,\n",
    "\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\tdrop_last=True)\n",
    "                        \n",
    "temp_data_iter = iter(temp_data_loader)\n",
    "\n",
    "timesteps, states, next_states, actions, rewards, traj_mask = next(temp_data_iter)\n",
    "\n",
    "timesteps = timesteps.to(DEVICE)\t# B x T\n",
    "states = states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "next_states = next_states.to(DEVICE) # B X T X state_dim\n",
    "actions = actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "rewards = rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "\n",
    "print(\"timesteps shape: \", timesteps.shape)\n",
    "print(\"rewards shape: \", rewards.shape)\n",
    "print(\"states shape: \", states.shape)\n",
    "print(\"actions shape: \", actions.shape)\n",
    "\n",
    "# print(\"state: \", states)\n",
    "# print(\"action: \", actions)\n",
    "# print(\"rewards: \", rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "\n",
    "temp_model = DecisionTransformer(\n",
    "\t\t\tstate_dim=state_dim,\n",
    "\t\t\tact_dim=act_dim,\n",
    "\t\t\t# reward 포함 + r0 제외\n",
    "\t\t\tn_blocks=n_blocks,\n",
    "\t\t\th_dim=16,\n",
    "\t\t\tcontext_len=2,\n",
    "\t\t\tn_heads=n_heads,\n",
    "\t\t\tdrop_p=drop_out,\n",
    "\t\t).to(DEVICE)\n",
    "\t\t\n",
    "next_state_preds, rewards_preds = temp_model.forward(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\trewards=rewards,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=timesteps,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tstates=states,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tactions=actions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "model = DecisionTransformer(\n",
    "\t\t\tstate_dim=state_dim,\n",
    "\t\t\tact_dim=act_dim,\n",
    "\t\t\tn_blocks=n_blocks,\n",
    "\t\t\th_dim=embed_dim,\n",
    "\t\t\tcontext_len=k,\n",
    "\t\t\tn_heads=n_heads,\n",
    "\t\t\tdrop_p=drop_out,\n",
    "\t\t).to(DEVICE)\n",
    "  \n",
    "optimizer = torch.optim.AdamW(\n",
    "\t\t\t\t\tmodel.parameters(), \n",
    "\t\t\t\t\tlr=lr, \n",
    "\t\t\t\t\tweight_decay=wt_decay\n",
    "\t\t\t\t)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "\t\toptimizer,\n",
    "\t\tlambda steps: min((steps+1)/warmup_steps, 1)\n",
    "\t)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m next_states_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone(next_states)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     22\u001b[0m rewards_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone(rewards)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 24\u001b[0m next_state_preds, rewards_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t\t\t\t\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t\t\t\t\u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t\t\t\t\u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t\t\t\t\u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t\t\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# only consider non padded elements\u001b[39;00m\n\u001b[0;32m     32\u001b[0m next_state_preds \u001b[38;5;241m=\u001b[39m next_state_preds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, state_dim)[traj_mask\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Develop\\offlineRL-with-diffusion\\transformer\\model.py:149\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[1;34m(self, timesteps, rewards, states, actions)\u001b[0m\n\u001b[0;32m    146\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_ln(h)\n\u001b[0;32m    148\u001b[0m \u001b[39m# transformer and prediction\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(h)\n\u001b[0;32m    151\u001b[0m \u001b[39m# get h reshaped such that its size = (B x 3 x T x h_dim) and\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[39m# h[:, 0, t] is conditioned on the input sequence r_0, s_0, a_0 ... r_t\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m# h[:, 1, t] is conditioned on the input sequence r_0, s_0, a_0 ... r_t, s_t\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m# -> (s_t, a_t, r_t)\u001b[39;00m\n\u001b[0;32m    160\u001b[0m h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mreshape(B, T, \u001b[39m3\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh_dim)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Develop\\offlineRL-with-diffusion\\transformer\\model.py:74\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(x) \u001b[39m# residual\u001b[39;00m\n\u001b[0;32m     73\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(x)\n\u001b[1;32m---> 74\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(x) \u001b[39m# residual\u001b[39;00m\n\u001b[0;32m     75\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x)\n\u001b[0;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\zkdlx\\miniconda3\\envs\\rl_diffusion\\lib\\site-packages\\torch\\nn\\functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1294\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1295\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "prefix = \"dt_\" + env_name\n",
    "\n",
    "save_model_name =  prefix + \"_model_\" + start_time_str + \".pt\"\n",
    "save_model_path = os.path.join(LOG_PATH, save_model_name)\n",
    "save_best_model_path = save_model_path[:-3] + \"_best.pt\"\n",
    "\n",
    "log_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
    "log_csv_path = os.path.join(LOG_PATH, log_csv_name)\n",
    "\n",
    "\n",
    "csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
    "csv_header = ([\"duration\", \"num_updates\", \"total_loss\", \"state_loss\", \"reward_loss\", \n",
    "               \"eval_avg_reward\", \"eval_avg_ep_len\", \"eval_d4rl_score\"])\n",
    "\n",
    "csv_writer.writerow(csv_header)\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"start time: \" + start_time_str)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"device set to: \" + str(DEVICE))\n",
    "print(\"dataset path: \" + DATA_PATH)\n",
    "print(\"model save path: \" + save_model_path)\n",
    "print(\"log csv save path: \" + log_csv_path)\n",
    "\n",
    "# train\n",
    "for i_train_iter in range(max_train_iters):\n",
    "\n",
    "\n",
    "\tlog_state_losses, log_reward_losses, log_total_losses = [], [], []\n",
    "\tmodel.train()\n",
    " \n",
    "\tfor _ in range(num_updates_per_iter):\n",
    "\t\ttry:\n",
    "\t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(data_iter)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\tdata_iter = iter(traj_data_loader)\n",
    "\t\t\ttimesteps, states, next_states, actions, rewards, traj_mask = next(data_iter)\n",
    "\n",
    "\t\ttimesteps = timesteps.to(DEVICE)\t# B x T\n",
    "\t\tstates = states.to(DEVICE)\t\t\t# B x T x state_dim\n",
    "\t\tnext_states = next_states.to(DEVICE) # B X T X state_dim\n",
    "\t\tactions = actions.to(DEVICE)\t\t# B x T x act_dim\n",
    "\t\trewards = rewards.to(DEVICE).unsqueeze(dim=-1) # B x T x 1\n",
    "\t\ttraj_mask = traj_mask.to(DEVICE)\t# B x T\n",
    "\n",
    "\t\tnext_states_target = torch.clone(next_states).detach().to(DEVICE)\n",
    "\t\trewards_target = torch.clone(rewards).detach().to(DEVICE)\n",
    "\t\n",
    "\t\tnext_state_preds, rewards_preds = model.forward(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=timesteps,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tstates=states,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactions=actions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\trewards=rewards,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\t\t# only consider non padded elements\n",
    "\t\tnext_state_preds = next_state_preds.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\tnext_states_target = next_states_target.view(-1, state_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\t\n",
    "\t\trewards_preds = rewards_preds.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\t\trewards_target = rewards_target.view(-1, 1)[traj_mask.view(-1,) > 0]\n",
    "\n",
    "\t\tstate_loss = F.mse_loss(next_state_preds, next_states_target, reduction='mean') * state_weight\n",
    "\t\treward_loss = F.mse_loss(rewards_preds, rewards_target, reduction='mean') * reward_weight\n",
    "\t\t\n",
    "\t\ttotal_loss = state_loss.add(reward_loss)\n",
    "\t\ttotal_loss = torch.mean(total_loss)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ttotal_loss.backward()\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#save loss\n",
    "\t\tlog_state_losses.append(state_loss.detach().cpu().item())\n",
    "\t\tlog_reward_losses.append(reward_loss.detach().cpu().item())\n",
    "\t\t\n",
    "\t\tlog_total_losses.append(total_loss.detach().cpu().item())\n",
    "\t\t\n",
    "\t# evaluate on env\n",
    "\tresults = evaluate_on_env(model, DEVICE, k, env, num_eval_ep, max_eval_ep_len, state_mean, state_std)\n",
    "\teval_avg_reward = results['eval/avg_reward']\n",
    "\teval_avg_ep_len = results['eval/avg_ep_len']\n",
    "\teval_d4rl_score = get_d4rl_normalized_score(results['eval/avg_reward'], env_name) * 100\n",
    "\n",
    "\tmean_total_log_loss = np.mean(log_total_losses)\n",
    "\tmean_state_log_loss = np.mean(log_state_losses)\n",
    "\tmean_reward_log_loss = np.mean(log_reward_losses)\n",
    "\t\n",
    "\ttime_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
    "\n",
    "\ttotal_updates += num_updates_per_iter\n",
    "\n",
    "\tlog_str = (\"=\" * 60 + '\\n' +\n",
    "\t\t\t\"time elapsed: \" + time_elapsed  + '\\n' +\n",
    "\t\t\t\"num of updates: \" + str(total_updates) + '\\n' +\n",
    "\t\t\t\"total loss: \" + format(mean_total_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"state loss: \" + format(mean_state_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"reward loss: \" +  format(mean_reward_log_loss, \".5f\") + '\\n' +\n",
    "\t\t\t\"eval avg reward: \" + format(eval_avg_reward, \".5f\") + '\\n' +\n",
    "\t\t\t\"eval avg ep len: \" + format(eval_avg_ep_len, \".5f\") + '\\n' +\n",
    "\t\t\t\"eval d4rl score: \" + format(eval_d4rl_score, \".5f\")\n",
    "\t\t\t)\n",
    "\n",
    "\tprint(log_str)\n",
    "\n",
    "\tlog_data = [time_elapsed, total_updates, mean_total_log_loss,\n",
    "\t\t\t\teval_avg_reward, eval_avg_ep_len,\n",
    "\t\t\t\teval_d4rl_score]\n",
    "\n",
    "\tcsv_writer.writerow(log_data)\n",
    "\t\n",
    "\t# save model\n",
    "\tprint(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "\tif eval_d4rl_score >= max_d4rl_score:\n",
    "\t\tprint(\"saving max d4rl score model at: \" + save_best_model_path)\n",
    "\t\ttorch.save(model.state_dict(), save_best_model_path)\n",
    "\t\tmax_d4rl_score = eval_d4rl_score\n",
    "\n",
    "\tprint(\"saving current model at: \" + save_model_path)\n",
    "\ttorch.save(model.state_dict(), save_model_path)\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"finished training!\")\n",
    "print(\"=\" * 60)\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "time_elapsed = str(end_time - start_time)\n",
    "end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "print(\"started training at: \" + start_time_str)\n",
    "print(\"finished training at: \" + end_time_str)\n",
    "print(\"total training time: \" + time_elapsed)\n",
    "print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "print(\"saved max d4rl score model at: \" + save_best_model_path)\n",
    "print(\"saved last updated model at: \" + save_model_path)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "csv_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('rl_diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaa54a55e816925fdb1964dae2307e16b21383867fc9aac098dfe4376e4c067b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
